{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f08fcc-0cef-445b-9395-12ce02eae6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assuming xml_file_path and df_wells_final are defined from your previous successful code\n",
    "\n",
    "# Define your Unity Catalog schema and table names\n",
    "catalog_name = \"harrison_chen_catalog\"\n",
    "schema_name = \"valnav_bronze\" # New schema for bronze layer ValNav data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e2349ce-0f52-4769-8e4c-71c570219951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook Code Cell\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, DateType\n",
    "\n",
    "# 1. Define the Unity Catalog Volume path to your XML file\n",
    "xml_file_path = \"/Volumes/harrison_chen_catalog/synthetic_energy/energy_volume/demo-reserve-data-very-large.xml\"\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # --- CRITICAL CHANGE HERE: Read the overall ROOT tag \"ProjectData\" ---\n",
    "    # This DataFrame will have a single row, containing all top-level XML elements as columns.\n",
    "    df_root_data = (spark.read\n",
    "      .format(\"xml\")\n",
    "      .option(\"rowTag\", \"ProjectData\") # Now reading the actual root tag of your XML\n",
    "      .option(\"attributePrefix\", \"_\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .load(xml_file_path)\n",
    "    )\n",
    "    print(f\"Successfully read XML from: {xml_file_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while reading or processing the XML file: {e}\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(f\"- The XML file exists at: {xml_file_path}\")\n",
    "    print(\"- You have READ VOLUME permission on the volume.\")\n",
    "    print(\"- The XML file is well-formed and matches the expected structure.\")\n",
    "    print(\"- The 'rowTag' (now 'ProjectData') correctly identifies the root of your XML.\")\n",
    "    print(\"- The nested paths like 'WellsAndGroups.Well' match your XML structure exactly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e991f03f-5bd7-4b8f-b7ed-f9807340d33c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    # --- Step 2: Access the nested 'WellsAndGroups' struct ---\n",
    "    # Since df_root_data has only one row, we can just select the specific struct column.\n",
    "    # The 'WellsAndGroups' column will itself be a struct, which contains an array of 'Well' structs.\n",
    "    df_wells_and_groups_struct = df_root_data.select(\"WellsAndGroups\")\n",
    "\n",
    "    print(\"\\nSchema of 'WellsAndGroups' struct (still single row):\")\n",
    "    df_wells_and_groups_struct.printSchema()\n",
    "    df_wells_and_groups_struct.display()\n",
    "\n",
    "\n",
    "    # --- Step 3: Explode the 'Well' array *within* the 'WellsAndGroups' struct ---\n",
    "    # The 'Well' column (which is an array) is nested inside the 'WellsAndGroups' struct.\n",
    "    # So, we access it as 'WellsAndGroups.Well'.\n",
    "    df_wells = df_wells_and_groups_struct.select(F.explode(\"WellsAndGroups.Well\").alias(\"WellData\"))\n",
    "\n",
    "    print(\"\\nSchema after exploding 'Well' (now one row per Well):\")\n",
    "    df_wells.printSchema()\n",
    "    df_wells.display() # You should now see multiple rows, one per <Well> element\n",
    "\n",
    "    # --- Step 4: Select and cast the specific columns from the exploded 'WellData' struct ---\n",
    "    # This part remains similar to before, but now operates on the 'WellData' alias.\n",
    "    df_wells_final = df_wells.select(\n",
    "        F.col(\"WellData._ID\").alias(\"WellID\"),\n",
    "        F.col(\"WellData._Name\").alias(\"WellName\"),\n",
    "        F.col(\"WellData._Type\").alias(\"WellType\"),\n",
    "        F.col(\"WellData._FacilityID\").alias(\"FacilityID\"),\n",
    "        F.col(\"WellData._SpudDate\").alias(\"SpudDate\"), # Assuming this can be cast by inferSchema\n",
    "        F.col(\"WellData._Status\").alias(\"Status\"),\n",
    "        F.col(\"WellData._CurrentOilRate\").cast(DoubleType()).alias(\"CurrentOilRate\"),\n",
    "        F.col(\"WellData._CurrentGasRate\").cast(DoubleType()).alias(\"CurrentGasRate\"),\n",
    "        # Accessing nested elements like WellboreData and ReservoirData\n",
    "        F.col(\"WellData.WellboreData._Depth\").cast(IntegerType()).alias(\"WellboreDepth\"),\n",
    "        F.col(\"WellData.WellboreData._Trajectory\").alias(\"WellboreTrajectory\"),\n",
    "        F.col(\"WellData.ReservoirData._Formation\").alias(\"ReservoirFormation\"),\n",
    "        F.col(\"WellData.ReservoirData._FluidType\").alias(\"ReservoirFluidType\")\n",
    "    )\n",
    "\n",
    "    print(\"\\nFinal DataFrame Schema (for individual 'Well' records):\")\n",
    "    df_wells_final.printSchema()\n",
    "    print(\"\\nFinal DataFrame Data (for individual 'Well' records):\")\n",
    "    df_wells_final.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38dccc89-270e-46f6-ab77-61f30fc45843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"wells\"\n",
    "\n",
    "full_table_path = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "# --- Create the new schema if it doesn't exist ---\n",
    "# This command needs to be run once in an SQL cell or using spark.sql()\n",
    "# Note: You need CREATE SCHEMA permission on the catalog.\n",
    "print(f\"Ensuring schema {catalog_name}.{schema_name} exists...\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "print(f\"Schema {catalog_name}.{schema_name} is ready.\")\n",
    "\n",
    "# --- Save df_wells_final to Unity Catalog ---\n",
    "try:\n",
    "    print(f\"\\nSaving df_wells_final to Unity Catalog table: {full_table_path}...\")\n",
    "    (df_wells_final.write\n",
    "      .mode(\"overwrite\") # Overwrite the table if it already exists (useful for re-runs)\n",
    "      .saveAsTable(full_table_path)\n",
    "    )\n",
    "    print(f\"Successfully saved df_wells_final to {full_table_path}.\")\n",
    "\n",
    "    # Verify by reading it back\n",
    "    # df_wells_from_uc = spark.read.table(full_table_path)\n",
    "    # print(\"\\nVerifying data read from Unity Catalog:\")\n",
    "    # df_wells_from_uc.display()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving df_wells_final to Unity Catalog: {e}\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(f\"- You have CREATE TABLE permission on {catalog_name}.{schema_name}.\")\n",
    "    print(\"- There are no issues with the DataFrame schema or data types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ba8bee-d06d-4284-b6a0-23f72a7a80b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"facilities\"\n",
    "\n",
    "# Facilities Table\n",
    "print(\"\\n--- Processing Facilities ---\")\n",
    "try:\n",
    "    df_facilities_raw = df_root_data.select(\"Facilities\")\n",
    "    df_facilities_exploded = df_facilities_raw.select(F.explode(\"Facilities.Facility\").alias(\"FacilityData\"))\n",
    "\n",
    "    df_facilities = df_facilities_exploded.select(\n",
    "        F.col(\"FacilityData._ID\").alias(\"FacilityID\"),\n",
    "        F.col(\"FacilityData._Name\").alias(\"FacilityName\"),\n",
    "        F.col(\"FacilityData._Type\").alias(\"FacilityType\"),\n",
    "        F.col(\"FacilityData._Location\").alias(\"Location\"),\n",
    "        F.col(\"FacilityData._Latitude\").cast(DoubleType()).alias(\"Latitude\"),\n",
    "        F.col(\"FacilityData._Longitude\").cast(DoubleType()).alias(\"Longitude\"),\n",
    "        F.col(\"FacilityData.Capacity._VALUE\").cast(DoubleType()).alias(\"Capacity\"), # Text content of Capacity tag\n",
    "        F.col(\"FacilityData.Capacity._Unit\").alias(\"CapacityUnit\"), # Attribute of Capacity tag\n",
    "        F.col(\"FacilityData.Status\").alias(\"Status\")\n",
    "    )\n",
    "\n",
    "    full_table_path = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "    # --- Create the new schema if it doesn't exist ---\n",
    "    # This command needs to be run once in an SQL cell or using spark.sql()\n",
    "    # Note: You need CREATE SCHEMA permission on the catalog.\n",
    "    print(f\"Ensuring schema {catalog_name}.{schema_name} exists...\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "    print(f\"Schema {catalog_name}.{schema_name} is ready.\")\n",
    "\n",
    "    df_facilities.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.facilities\")\n",
    "    print(f\"Successfully saved {catalog_name}.{schema_name}.facilities\")\n",
    "    df_facilities.display()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing Facilities: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "877d3e7c-c499-4226-8370-0f8447bfb8bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"scenarios\"\n",
    "\n",
    "# COMMAND ----------\n",
    "# Scenarios Table (Alternative, more robust processing for scenarios)\n",
    "print(\"\\n--- Processing Scenarios (Direct Read Method) ---\")\n",
    "try:\n",
    "    # Instead of selecting from df_root_data and exploding,\n",
    "    # read the 'Scenario' elements directly from the XML file.\n",
    "    # Spark XML will find all <Scenario> tags anywhere in the document and treat each as a row.\n",
    "    df_scenarios_direct = (spark.read\n",
    "      .format(\"xml\")\n",
    "      .option(\"rowTag\", \"Scenario\") # Make <Scenario> the rowTag directly\n",
    "      .option(\"attributePrefix\", \"_\")\n",
    "      .option(\"inferSchema\", \"true\") # Let it infer schema for Scenario attributes and content\n",
    "      .load(xml_file_path) # Read from the main XML file path\n",
    "    )\n",
    "\n",
    "    # Now, simply select and rename columns from this directly read DataFrame\n",
    "    # There's no need for explode here as each row is already a Scenario\n",
    "    df_scenarios = df_scenarios_direct.select(\n",
    "        F.col(\"_ID\").alias(\"ScenarioID\"),\n",
    "        F.col(\"_Name\").alias(\"ScenarioName\"),\n",
    "        F.col(\"_PriceDeckID\").alias(\"PriceDeckID\"),\n",
    "        F.col(\"_Type\").alias(\"Type\"),\n",
    "        F.col(\"_Status\").alias(\"Status\"),\n",
    "        F.col(\"Description\").alias(\"Description\") # Assuming Description is a text element under <Scenario>\n",
    "    )\n",
    "\n",
    "    df_scenarios.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.scenarios\")\n",
    "    print(f\"Successfully saved {catalog_name}.{schema_name}.scenarios (via direct read)\")\n",
    "    df_scenarios.display()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing Scenarios (Direct Read Method): {e}\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(f\"- The XML file exists at: {xml_file_path}\")\n",
    "    print(\"- You have READ VOLUME permission on the volume.\")\n",
    "    print(\"- The 'rowTag' ('Scenario') matches an actual repeating element in your XML.\")\n",
    "    print(\"- The XML file is well-formed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7b0c5b-11e8-405e-9f33-62b5995bcf8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"price_decks\"\n",
    "\n",
    "# COMMAND ----------\n",
    "# Price Decks Tables (split into two: metadata and annual prices)\n",
    "print(\"\\n--- Processing Price Decks (Direct Read Method) ---\")\n",
    "try:\n",
    "    # --- CRITICAL CHANGE HERE: Read 'PriceDeck' directly as the rowTag ---\n",
    "    # This will create a DataFrame where each row is a <PriceDeck> element.\n",
    "    df_price_decks_direct = (spark.read\n",
    "      .format(\"xml\")\n",
    "      .option(\"rowTag\", \"PriceDeck\") # Direct read of PriceDeck elements\n",
    "      .option(\"attributePrefix\", \"_\")\n",
    "      .option(\"inferSchema\", \"true\") # Let it infer schema for PriceDeck attributes and nested elements\n",
    "      .load(xml_file_path) # Read from the main XML file path\n",
    "    )\n",
    "\n",
    "    print(\"\\nSchema after direct read of 'PriceDeck':\")\n",
    "    df_price_decks_direct.printSchema()\n",
    "    df_price_decks_direct.display()\n",
    "\n",
    "    # Table 1: Price Deck Metadata\n",
    "    df_price_decks_metadata = df_price_decks_direct.select(\n",
    "        F.col(\"_ID\").alias(\"PriceDeckID\"),\n",
    "        F.col(\"_Name\").alias(\"PriceDeckName\"),\n",
    "        F.col(\"_CurrencyID\").alias(\"CurrencyID\")\n",
    "    )\n",
    "    df_price_decks_metadata.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.price_decks_metadata\")\n",
    "    print(f\"Successfully saved {catalog_name}.{schema_name}.price_decks_metadata\")\n",
    "    df_price_decks_metadata.display()\n",
    "\n",
    "    # Table 2: Price Deck Annual Prices (requires two levels of explode from df_price_decks_direct)\n",
    "    # df_price_decks_direct should now have a 'PriceCommodity' column that is an ARRAY<STRUCT>\n",
    "    df_price_commodities_exploded = df_price_decks_direct.select(\n",
    "        F.col(\"_ID\").alias(\"PriceDeckID\"), # Carry PriceDeckID down\n",
    "        F.explode(\"PriceCommodity\").alias(\"PriceCommodityData\") # Explode the PriceCommodity array\n",
    "    )\n",
    "\n",
    "    # df_price_commodities_exploded should now have a 'PriceCommodityData.AnnualPrice' column that is an ARRAY<STRUCT>\n",
    "    df_price_annual_prices = df_price_commodities_exploded.select(\n",
    "        F.col(\"PriceDeckID\"),\n",
    "        F.col(\"PriceCommodityData._Commodity\").alias(\"Commodity\"),\n",
    "        F.col(\"PriceCommodityData._Unit\").alias(\"Unit\"),\n",
    "        F.explode(\"PriceCommodityData.AnnualPrice\").alias(\"AnnualPriceData\") # Explode the AnnualPrice array\n",
    "    ).select(\n",
    "        F.col(\"PriceDeckID\"),\n",
    "        F.col(\"Commodity\"),\n",
    "        F.col(\"Unit\"),\n",
    "        F.col(\"AnnualPriceData._Year\").cast(IntegerType()).alias(\"Year\"),\n",
    "        F.col(\"AnnualPriceData._Value\").cast(DoubleType()).alias(\"Value\")\n",
    "    )\n",
    "    df_price_annual_prices.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.price_decks_annual_prices\")\n",
    "    print(f\"Successfully saved {catalog_name}.{schema_name}.price_decks_annual_prices\")\n",
    "    df_price_annual_prices.display()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing Price Decks: {e}\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(f\"- The XML file exists at: {xml_file_path}\")\n",
    "    print(\"- You have READ VOLUME permission on the volume.\")\n",
    "    print(\"- The 'rowTag' ('PriceDeck') matches an actual repeating element in your XML.\")\n",
    "    print(\"- The XML file is well-formed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6a05511-1b1c-4b1c-a5ec-0bbe5dbd3369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"bulk_well_schedules\"\n",
    "\n",
    "# COMMAND ----------\n",
    "# Bulk Well Schedules Table\n",
    "print(\"\\n--- Processing Bulk Well Schedules ---\")\n",
    "try:\n",
    "    df_schedules_raw = df_root_data.select(\"BulkWellSchedules\")\n",
    "    df_schedules_exploded = df_schedules_raw.select(F.explode(\"BulkWellSchedules.WellSchedule\").alias(\"WellScheduleData\"))\n",
    "\n",
    "    df_production_entries = df_schedules_exploded.select(\n",
    "        F.col(\"WellScheduleData._WellID\").alias(\"WellID\"), # Carry WellID down\n",
    "        F.col(\"WellScheduleData._ScenarioID\").alias(\"ScenarioID\"), # Carry ScenarioID down\n",
    "        F.explode(\"WellScheduleData.ProductionEntry\").alias(\"ProductionEntryData\")\n",
    "    )\n",
    "\n",
    "    df_bulk_well_schedules = df_production_entries.select(\n",
    "        F.col(\"WellID\"),\n",
    "        F.col(\"ScenarioID\"),\n",
    "        F.col(\"ProductionEntryData._Date\").cast(DateType()).alias(\"ProductionDate\"),\n",
    "        F.col(\"ProductionEntryData._OilRate\").cast(DoubleType()).alias(\"OilRate\"),\n",
    "        F.col(\"ProductionEntryData._GasRate\").cast(DoubleType()).alias(\"GasRate\"),\n",
    "        F.col(\"ProductionEntryData._WaterRate\").cast(DoubleType()).alias(\"WaterRate\")\n",
    "    )\n",
    "    df_bulk_well_schedules.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.bulk_well_schedules\")\n",
    "    print(f\"Successfully saved {catalog_name}.{schema_name}.bulk_well_schedules\")\n",
    "    df_bulk_well_schedules.display()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing Bulk Well Schedules: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0809f645-b77b-4291-9925-9fc894a2a259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"currencies\"\n",
    "\n",
    "# Currencies Table\n",
    "print(\"\\n--- Processing Currencies ---\")\n",
    "try:\n",
    "    df_currencies_raw = df_root_data.select(\"Currencies\")\n",
    "    df_currencies_exploded = df_currencies_raw.select(F.explode(\"Currencies.Currency\").alias(\"CurrencyData\"))\n",
    "\n",
    "    df_currencies = df_currencies_exploded.select(\n",
    "        F.col(\"CurrencyData._ID\").alias(\"CurrencyID\"),\n",
    "        F.col(\"CurrencyData._Name\").alias(\"CurrencyName\"),\n",
    "        F.col(\"CurrencyData._Symbol\").alias(\"Symbol\")\n",
    "    )\n",
    "    df_currencies.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.currencies\")\n",
    "    print(f\"Successfully saved {catalog_name}.{schema_name}.currencies\")\n",
    "    df_currencies.display()\n",
    "except Exception as e:\n",
    "    print(f\"Error processing Currencies: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# Countries Table\n",
    "print(\"\\n--- Processing Countries ---\")\n",
    "try:\n",
    "    df_countries_raw = df_root_data.select(\"Countries\")\n",
    "    df_countries_exploded = df_countries_raw.select(F.explode(\"Countries.Country\").alias(\"CountryData\"))\n",
    "\n",
    "    df_countries = df_countries_exploded.select(\n",
    "        F.col(\"CountryData._ID\").alias(\"CountryID\"),\n",
    "        F.col(\"CountryData._Name\").alias(\"CountryName\")\n",
    "    )\n",
    "    df_countries.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.countries\")\n",
    "    print(f\"Successfully saved {catalog_name}.{schema_name}.countries\")\n",
    "    df_countries.display()\n",
    "except Exception as e:\n",
    "    print(f\"Error processing Countries: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1023fa9-b163-4ead-b6b7-19bd3fd231a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Synthetic Well Data XML Generator Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2cae3f-fa3d-44a6-a3e9-9cea4002876b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_synthetic_valnav_xml(num_wells=250, num_facilities=15):\n",
    "    \"\"\"\n",
    "    Generates a synthetic XML dataset replicating oil reserve evaluation data.\n",
    "\n",
    "    Args:\n",
    "        num_wells (int): The number of unique well IDs to generate.\n",
    "        num_facilities (int): The number of unique facility IDs to generate.\n",
    "                              Wells will be randomly assigned to these facilities.\n",
    "\n",
    "    Returns:\n",
    "        str: A well-formed XML string containing the synthetic data.\n",
    "    \"\"\"\n",
    "    current_date = datetime.now()\n",
    "    xml_parts = []\n",
    "\n",
    "    # XML Declaration and Root\n",
    "    xml_parts.append('<?xml version=\"1.0\" encoding=\"UTF-8\"?>')\n",
    "    xml_parts.append(f'<ProjectData generatedBy=\"DatabricksSynth\" exportDate=\"{current_date.isoformat()}Z\">')\n",
    "\n",
    "    # --- PriceDecks (unchanged for simplicity, but could be extended similarly) ---\n",
    "    xml_parts.append('  <PriceDecks>')\n",
    "    xml_parts.append('    <PriceDeck ID=\"PD-001\" Name=\"Base Case Prices 2025-2030\" CurrencyID=\"USD\">')\n",
    "    xml_parts.append('      <PriceCommodity Commodity=\"Oil\" Unit=\"USD/bbl\">')\n",
    "    for y in range(2025, 2031):\n",
    "        xml_parts.append(f'        <AnnualPrice Year=\"{y}\" Value=\"{random.uniform(65.0, 78.0):.2f}\"/>')\n",
    "    xml_parts.append('      </PriceCommodity>')\n",
    "    xml_parts.append('      <PriceCommodity Commodity=\"Gas\" Unit=\"USD/Mcf\">')\n",
    "    for y in range(2025, 2031):\n",
    "        xml_parts.append(f'        <AnnualPrice Year=\"{y}\" Value=\"{random.uniform(2.5, 3.5):.2f}\"/>')\n",
    "    xml_parts.append('      </PriceCommodity>')\n",
    "    xml_parts.append('    </PriceDeck>')\n",
    "    xml_parts.append('  </PriceDecks>')\n",
    "\n",
    "    # --- Scenarios (unchanged for simplicity) ---\n",
    "    xml_parts.append('  <Scenarios>')\n",
    "    xml_parts.append('    <Scenario ID=\"SCN-001\" Name=\"P50 Base Economic Forecast\" PriceDeckID=\"PD-001\" Type=\"Deterministic\" Status=\"Active\">')\n",
    "    xml_parts.append('      <Description>Primary forecast using base case prices.</Description>')\n",
    "    xml_parts.append('    </Scenario>')\n",
    "    xml_parts.append('  </Scenarios>')\n",
    "\n",
    "    # --- Facilities (Generate more, with varied data) ---\n",
    "    facility_ids = []\n",
    "    facility_types = [\"Processing Unit\", \"Gathering Station\", \"Gas Plant\", \"Terminal\"]\n",
    "    facility_statuses = [\"Operational\", \"Under Maintenance\", \"Under Construction\"]\n",
    "    xml_parts.append('  <Facilities>')\n",
    "    for i in range(1, num_facilities + 1):\n",
    "        fac_id = f\"FAC-{i:03d}\" # e.g., FAC-001, FAC-002\n",
    "        facility_ids.append(fac_id)\n",
    "        fac_name = f\"Field {chr(64 + random.randint(1, 26))} Unit {i}\"\n",
    "        fac_type = random.choice(facility_types)\n",
    "        location_name = f\"Basin {chr(64 + random.randint(1, 10))}\"\n",
    "        latitude = round(random.uniform(25.0, 60.0), 2)\n",
    "        longitude = round(random.uniform(-125.0, -60.0), 2)\n",
    "        capacity = random.randint(5000, 200000) # Increased capacity range\n",
    "        status = random.choice(facility_statuses)\n",
    "\n",
    "        xml_parts.append(f'    <Facility ID=\"{fac_id}\" Name=\"{fac_name}\" Type=\"{fac_type}\" Location=\"{location_name}\" Latitude=\"{latitude}\" Longitude=\"{longitude}\">')\n",
    "        xml_parts.append(f'      <Capacity Unit=\"Bbl/day\">{capacity}</Capacity>')\n",
    "        xml_parts.append(f'      <Status>{status}</Status>')\n",
    "        xml_parts.append('    </Facility>')\n",
    "    xml_parts.append('  </Facilities>')\n",
    "\n",
    "    # --- WellsAndGroups (assign to newly generated facilities) ---\n",
    "    xml_parts.append('  <WellsAndGroups>')\n",
    "    well_ids = []\n",
    "    well_types = [\"Oil Producer\", \"Gas Producer\", \"Injector\"]\n",
    "    well_statuses = [\"Producing\", \"Shut-in\", \"Drilling\", \"Abandoned\"]\n",
    "    trajectories = [\"Horizontal\", \"Vertical\", \"Long Reach\", \"Deviated\"]\n",
    "    formations = [\"Wolfcamp\", \"Bakkens\", \"Montney\", \"Eagle Ford\", \"Permian Shale\", \"Marcellus\"]\n",
    "    fluid_types = [\"Oil\", \"Gas\", \"Water\", \"Condensate\"]\n",
    "\n",
    "    for i in range(1, num_wells + 1):\n",
    "        well_id = f\"WELL-{(i):04d}\"\n",
    "        well_ids.append(well_id)\n",
    "        well_name = f\"Well {random.choice(['Alpha', 'Beta', 'Gamma', 'Delta'])}-{i:03d}\"\n",
    "        facility_id = random.choice(facility_ids) # Assign to one of the generated facilities\n",
    "        spud_date = (current_date - timedelta(days=random.randint(365, 365*20))).strftime('%Y-%m-%d') # Up to 20 years old\n",
    "        status = random.choice(well_statuses)\n",
    "        well_type = random.choice(well_types)\n",
    "\n",
    "        current_oil_rate = round(random.uniform(0, 500) if \"Oil\" in well_type else 0, 2)\n",
    "        current_gas_rate = round(random.uniform(0, 5) if \"Gas\" in well_type else 0, 2)\n",
    "        if well_type == \"Injector\":\n",
    "            current_oil_rate = 0\n",
    "            current_gas_rate = 0\n",
    "\n",
    "        wellbore_depth = random.randint(5000, 20000) # Deeper wells\n",
    "        trajectory = random.choice(trajectories)\n",
    "        reservoir_formation = random.choice(formations)\n",
    "        reservoir_fluid_type = random.choice(fluid_types)\n",
    "\n",
    "        xml_parts.append(f'    <Well ID=\"{well_id}\" Name=\"{well_name}\" Type=\"{well_type}\" FacilityID=\"{facility_id}\" SpudDate=\"{spud_date}\" Status=\"{status}\" CurrentOilRate=\"{current_oil_rate}\" CurrentGasRate=\"{current_gas_rate}\">')\n",
    "        xml_parts.append(f'      <WellboreData Depth=\"{wellbore_depth}\" Trajectory=\"{trajectory}\"/>')\n",
    "        xml_parts.append(f'      <ReservoirData Formation=\"{reservoir_formation}\" FluidType=\"{reservoir_fluid_type}\"/>')\n",
    "        xml_parts.append('    </Well>')\n",
    "\n",
    "    # Add a dummy group\n",
    "    xml_parts.append('    <Group ID=\"GRP-001\" Name=\"All Wells Test Group\" Type=\"Overall\">')\n",
    "    for wid in random.sample(well_ids, min(5, num_wells)):\n",
    "        xml_parts.append(f'      <MemberWellID>{wid}</MemberWellID>')\n",
    "    xml_parts.append('    </Group>')\n",
    "\n",
    "    xml_parts.append('  </WellsAndGroups>')\n",
    "\n",
    "    # --- BulkWellSchedules (more entries, more wells, more months) ---\n",
    "    xml_parts.append('  <BulkWellSchedules>')\n",
    "    # Generate schedules for a higher percentage of wells (e.g., 70% of wells)\n",
    "    schedules_for_wells = random.sample(well_ids, min(int(num_wells * 0.7), num_wells))\n",
    "    for wid in schedules_for_wells:\n",
    "        xml_parts.append(f'    <WellSchedule WellID=\"{wid}\" ScenarioID=\"SCN-001\">')\n",
    "        # Generate data for a longer period (e.g., last 12-36 months)\n",
    "        num_months_history = random.randint(12, 36)\n",
    "        for month_offset in range(num_months_history):\n",
    "            prod_date = (current_date - timedelta(days=30*month_offset)).strftime('%Y-%m-%d')\n",
    "            # Vary production rates more significantly\n",
    "            oil_prod = round(random.uniform(10, 300), 2)\n",
    "            gas_prod = round(random.uniform(0.05, 3.0), 2)\n",
    "            water_prod = round(random.uniform(0, 50), 2)\n",
    "            xml_parts.append(f'      <ProductionEntry Date=\"{prod_date}\" OilRate=\"{oil_prod}\" GasRate=\"{gas_prod}\" WaterRate=\"{water_prod}\"/>')\n",
    "        xml_parts.append('    </WellSchedule>')\n",
    "    xml_parts.append('  </BulkWellSchedules>')\n",
    "\n",
    "\n",
    "    # --- Currencies (add more) ---\n",
    "    xml_parts.append('  <Currencies>')\n",
    "    currencies_data = [\n",
    "        {\"ID\": \"USD\", \"Name\": \"United States Dollar\", \"Symbol\": \"$\"},\n",
    "        {\"ID\": \"CAD\", \"Name\": \"Canadian Dollar\", \"Symbol\": \"C$\"},\n",
    "        {\"ID\": \"EUR\", \"Name\": \"Euro\", \"Symbol\": \"€\"},\n",
    "        {\"ID\": \"GBP\", \"Name\": \"British Pound\", \"Symbol\": \"£\"},\n",
    "        {\"ID\": \"JPY\", \"Name\": \"Japanese Yen\", \"Symbol\": \"¥\"},\n",
    "        {\"ID\": \"AUD\", \"Name\": \"Australian Dollar\", \"Symbol\": \"A$\"},\n",
    "        {\"ID\": \"CNY\", \"Name\": \"Chinese Yuan\", \"Symbol\": \"¥\"},\n",
    "        {\"ID\": \"BRL\", \"Name\": \"Brazilian Real\", \"Symbol\": \"R$\"}\n",
    "    ]\n",
    "    for currency in currencies_data:\n",
    "        xml_parts.append(f'    <Currency ID=\"{currency[\"ID\"]}\" Name=\"{currency[\"Name\"]}\" Symbol=\"{currency[\"Symbol\"]}\"/>')\n",
    "    xml_parts.append('  </Currencies>')\n",
    "\n",
    "    # --- Countries (unchanged, but could be extended) ---\n",
    "    xml_parts.append('  <Countries>')\n",
    "    xml_parts.append('    <Country ID=\"US\" Name=\"United States\"/>')\n",
    "    xml_parts.append('    <Country ID=\"CA\" Name=\"Canada\"/>')\n",
    "    xml_parts.append('  </Countries>')\n",
    "\n",
    "    # --- End Root ---\n",
    "    xml_parts.append('</ProjectData>')\n",
    "\n",
    "    return \"\\n\".join(xml_parts)\n",
    "\n",
    "# --- Main execution block ---\n",
    "# Generate the XML data with more facilities and wells\n",
    "synthetic_xml_data = generate_synthetic_valnav_xml(num_wells=500, num_facilities=20) # Example: 500 wells, 20 facilities\n",
    "\n",
    "# --- Now, write this data to your Databricks Volume ---\n",
    "# Ensure you have WRITE VOLUME permission on the volume\n",
    "\n",
    "volume_file_path = \"/Volumes/harrison_chen_catalog/synthetic_energy/energy_volume/demo-reserve-data-very-large.xml\" # Use a new, distinct file name\n",
    "\n",
    "try:\n",
    "    # Write the XML string to the specified volume path\n",
    "    dbutils.fs.put(volume_file_path, synthetic_xml_data, overwrite=True)\n",
    "    print(f\"\\nSuccessfully wrote the large synthetic XML data to: {volume_file_path}\")\n",
    "    print(f\"File size: {len(synthetic_xml_data)} bytes. (Contains {500} wells and {20} facilities)\")\n",
    "\n",
    "    # You can verify by reading the head of the file\n",
    "    # print(\"\\nVerifying file content on volume:\")\n",
    "    # print(dbutils.fs.head(volume_file_path, 2000)) # Read more characters for larger file\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError writing XML to volume: {e}\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(f\"- The target volume path is correct: {volume_file_path}\")\n",
    "    print(\"- You have WRITE VOLUME permission on the volume.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c75093c8-0895-475e-8c1b-6acacfe3368b",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Untitled Notebook 2025-07-04 09_43_29",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
